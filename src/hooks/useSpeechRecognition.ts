// src/hooks/useSpeechRecognition.ts
import { useState, useRef, useEffect, useCallback } from 'react';

export interface SpeechRecognitionHook {
  isListening: boolean;
  transcript: string;
  interimTranscript: string;
  confidence: number;
  startListening: () => void;
  stopListening: () => void;
  resetTranscript: () => void;
  isSupported: boolean;
  error: string | null;
  hasPermission: boolean;
  isProcessing: boolean;
}

// Enhanced type definitions
type SpeechRecognitionResultItem = {
  transcript: string;
  confidence: number;
};

type SpeechRecognitionResult = {
  readonly isFinal: boolean;
  readonly 0: SpeechRecognitionResultItem;
  readonly length: number;
};

type SpeechRecognitionResultList = Array<SpeechRecognitionResult>;

interface ISpeechRecognition extends EventTarget {
  lang: string;
  maxAlternatives: number;
  continuous: boolean;
  interimResults: boolean;
  start(): void;
  stop(): void;
  abort(): void;
  onstart: ((this: ISpeechRecognition, ev: Event) => void) | null;
  onresult: ((this: ISpeechRecognition, ev: SpeechRecognitionEvent) => void) | null;
  onerror: ((this: ISpeechRecognition, ev: SpeechRecognitionErrorEvent) => void) | null;
  onend: ((this: ISpeechRecognition, ev: Event) => void) | null;
  onspeechstart: ((this: ISpeechRecognition, ev: Event) => void) | null;
  onspeechend: ((this: ISpeechRecognition, ev: Event) => void) | null;
  onaudiostart: ((this: ISpeechRecognition, ev: Event) => void) | null;
  onaudioend: ((this: ISpeechRecognition, ev: Event) => void) | null;
  onnomatch: ((this: ISpeechRecognition, ev: Event) => void) | null;
}

interface SpeechRecognitionEvent extends Event {
  readonly resultIndex: number;
  readonly results: SpeechRecognitionResultList;
}

interface SpeechRecognitionErrorEvent extends Event {
  readonly error: string;
  readonly message: string;
}

declare global {
  interface Window {
    SpeechRecognition?: {
      new (): ISpeechRecognition;
    };
    webkitSpeechRecognition?: {
      new (): ISpeechRecognition;
    };
  }
}

export const useSpeechRecognition = (): SpeechRecognitionHook => {
  const [isListening, setIsListening] = useState(false);
  const [transcript, setTranscript] = useState('');
  const [interimTranscript, setInterimTranscript] = useState('');
  const [confidence, setConfidence] = useState(0);
  const [error, setError] = useState<string | null>(null);
  const [hasPermission, setHasPermission] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);

  const recognitionRef = useRef<ISpeechRecognition | null>(null);
  const timeoutRef = useRef<NodeJS.Timeout | null>(null);
  const silenceTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const isInitializedRef = useRef(false);
  const hasSpeechDetectedRef = useRef(false);
  const finalTranscriptRef = useRef('');
  const isAutoRestartingRef = useRef(false);

  // Check microphone permission with better handling
  useEffect(() => {
    const checkPermission = async () => {
      try {
        if ('permissions' in navigator) {
          const permission = await navigator.permissions.query({ name: 'microphone' as PermissionName });
          setHasPermission(permission.state === 'granted');
          
          permission.onchange = () => {
            setHasPermission(permission.state === 'granted');
            if (permission.state === 'denied') {
              setError('Quyá»n truy cáº­p microphone Ä‘Ã£ bá»‹ tá»« chá»‘i. Vui lÃ²ng cáº¥p quyá»n trong cÃ i Ä‘áº·t trÃ¬nh duyá»‡t.');
            }
          };
        } else {
          // For browsers that don't support permissions API, try getUserMedia
          try {
            if (
              typeof navigator !== 'undefined' &&
              typeof (navigator as Navigator).mediaDevices !== 'undefined' &&
              (navigator as Navigator).mediaDevices &&
              typeof (navigator as Navigator).mediaDevices.getUserMedia === 'function'
            ) {
              const stream = await (navigator as Navigator).mediaDevices.getUserMedia({ audio: true });
              stream.getTracks().forEach((track: MediaStreamTrack) => track.stop());
              setHasPermission(true);
            } else {
              setHasPermission(false);
              setError('TrÃ¬nh duyá»‡t khÃ´ng há»— trá»£ truy cáº­p microphone.');
            }
          } catch {
            setHasPermission(false);
            setError('KhÃ´ng thá»ƒ truy cáº­p microphone. Vui lÃ²ng kiá»ƒm tra quyá»n truy cáº­p.');
          }
        }
      } catch {
        setHasPermission(true); // Fallback
      }
    };

    checkPermission();
  }, []);

  // Initialize speech recognition with better configuration
  useEffect(() => {
    if (isInitializedRef.current) return;

    const SpeechRecognitionClass =
      window.SpeechRecognition || window.webkitSpeechRecognition;
    
    if (!SpeechRecognitionClass) {
      setError('TrÃ¬nh duyá»‡t khÃ´ng há»— trá»£ Web Speech API. Vui lÃ²ng sá»­ dá»¥ng Chrome, Edge hoáº·c Safari.');
      return;
    }

    const recognition = new SpeechRecognitionClass();
    
    // Optimized settings for better Korean recognition
    recognition.lang = 'ko-KR';
    recognition.continuous = true; // Changed to true for longer speech
    recognition.interimResults = true;
    recognition.maxAlternatives = 5; // Increased for better accuracy

    recognition.onstart = () => {
      console.log('Speech recognition started');
      setIsListening(true);
      setIsProcessing(false);
      setError(null);
      
      // âš ï¸ CHá»ˆ reset khi KHÃ”NG pháº£i auto-restart
      if (!isAutoRestartingRef.current) {
        console.log('ðŸ†• Fresh start - clearing transcript');
        setInterimTranscript('');
        hasSpeechDetectedRef.current = false;
        finalTranscriptRef.current = '';
      } else {
        console.log('ðŸ”„ Auto-restart - keeping existing transcript:', finalTranscriptRef.current);
      }
      
      // Extended timeout for longer speech
      timeoutRef.current = setTimeout(() => {
        console.log('Recognition timeout - stopping');
        if (recognitionRef.current) {
          recognition.stop();
        }
      }, 30000); // Increased to 30 seconds
    };

    recognition.onaudiostart = () => {
      console.log('Audio capture started');
      setIsProcessing(true);
    };

    recognition.onspeechstart = () => {
      console.log('Speech detected');
      hasSpeechDetectedRef.current = true;
      setIsProcessing(true);
      
      // Clear initial timeout when speech is detected
      if (timeoutRef.current) {
        clearTimeout(timeoutRef.current);
        timeoutRef.current = null;
      }
    };

    recognition.onresult = (event: SpeechRecognitionEvent) => {
      console.log('Recognition result received', event);
      let finalTranscript = '';
      let interim = '';
      let maxConfidence = 0;

      for (let i = 0; i < event.results.length; i++) {
        const result = event.results[i];
        const resultTranscript = result[0].transcript;
        
        if (result.isFinal) {
          finalTranscript += resultTranscript;
          maxConfidence = Math.max(maxConfidence, result[0].confidence || 0);
          console.log('Final result:', resultTranscript);
        } else {
          interim += resultTranscript;
          console.log('Interim result:', resultTranscript);
        }
      }

      // Update interim results immediately
      if (interim) {
        setInterimTranscript(interim);
      }

      // Handle final results
      if (finalTranscript) {
        finalTranscriptRef.current += finalTranscript;
        setTranscript(finalTranscriptRef.current.trim());
        setConfidence(maxConfidence);
        setInterimTranscript('');
        
        // Start silence detection timer
        if (silenceTimeoutRef.current) {
          clearTimeout(silenceTimeoutRef.current);
        }
        
        silenceTimeoutRef.current = setTimeout(() => {
          console.log('Silence detected - stopping recognition');
          recognition.stop();
        }, 5000); // Stop after 5 seconds of silence (cho phÃ©p user nghÄ© lÃ¢u)
      }
    };

    recognition.onspeechend = () => {
      console.log('Speech ended');
      setIsProcessing(false);
      
      // Wait a bit more before stopping in case there's more speech
      if (silenceTimeoutRef.current) {
        clearTimeout(silenceTimeoutRef.current);
      }
      
      silenceTimeoutRef.current = setTimeout(() => {
        console.log('Speech end timeout - stopping');
        recognition.stop();
      }, 2000); // Chá» 2s sau khi phÃ¡t hiá»‡n speech end
    };

    recognition.onaudioend = () => {
      console.log('Audio capture ended');
      setIsProcessing(false);
    };

    recognition.onerror = (event: SpeechRecognitionErrorEvent) => {
      console.error('Speech recognition error:', event.error, event.message);
      
      let errorMessage = 'Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh';
      
      switch (event.error) {
        case 'not-allowed':
          errorMessage = 'KhÃ´ng cÃ³ quyá»n truy cáº­p microphone. Vui lÃ²ng cáº¥p quyá»n vÃ  thá»­ láº¡i.';
          setHasPermission(false);
          break;
        case 'no-speech':
          if (!hasSpeechDetectedRef.current) {
            errorMessage = 'KhÃ´ng nghe tháº¥y giá»ng nÃ³i. Vui lÃ²ng nÃ³i to hÆ¡n hoáº·c kiá»ƒm tra microphone.';
          } else {
            // âœ… ÄÃ£ cÃ³ speech trÆ°á»›c Ä‘Ã³ â†’ Tá»± Ä‘á»™ng restart Ä‘á»ƒ tiáº¿p tá»¥c láº¯ng nghe
            console.log('ðŸ”„ Auto-restarting recognition after no-speech...');
            isAutoRestartingRef.current = true;
            // âš ï¸ KHÃ”NG set isListening = false Ä‘á»ƒ giá»¯ nguyÃªn UI vÃ  text
            
            // Restart ngay láº­p tá»©c
            setTimeout(() => {
              if (recognitionRef.current) {
                try {
                  recognitionRef.current.start();
                  console.log('âœ… Recognition restarted - continuing from:', finalTranscriptRef.current);
                  isAutoRestartingRef.current = false;
                } catch (err) {
                  console.warn('Failed to restart:', err);
                  isAutoRestartingRef.current = false;
                  setIsListening(false);
                }
              }
            }, 300);
            return;
          }
          break;
        case 'audio-capture':
          errorMessage = 'KhÃ´ng thá»ƒ truy cáº­p microphone. Kiá»ƒm tra thiáº¿t bá»‹ Ã¢m thanh.';
          break;
        case 'network':
          errorMessage = 'Lá»—i máº¡ng. Kiá»ƒm tra káº¿t ná»‘i internet vÃ  thá»­ láº¡i.';
          break;
        case 'aborted':
          // Don't show error for user-initiated abort
          setIsListening(false);
          setIsProcessing(false);
          return;
        case 'language-not-supported':
          errorMessage = 'NgÃ´n ngá»¯ tiáº¿ng HÃ n khÃ´ng Ä‘Æ°á»£c há»— trá»£ trÃªn thiáº¿t bá»‹ nÃ y.';
          break;
        default:
          errorMessage = `Lá»—i nháº­n diá»‡n: ${event.error}`;
      }
      
      setError(errorMessage);
      setIsListening(false);
      setIsProcessing(false);
    };

    recognition.onend = () => {
      console.log('Recognition ended');
      setIsListening(false);
      setIsProcessing(false);
      
      // Clean up timeouts
      if (timeoutRef.current) {
        clearTimeout(timeoutRef.current);
        timeoutRef.current = null;
      }
      if (silenceTimeoutRef.current) {
        clearTimeout(silenceTimeoutRef.current);
        silenceTimeoutRef.current = null;
      }
    };

    // Handle no match
    recognition.onnomatch = () => {
      console.log('No match found');
      if (!hasSpeechDetectedRef.current) {
        setError('KhÃ´ng nháº­n diá»‡n Ä‘Æ°á»£c giá»ng nÃ³i. Vui lÃ²ng nÃ³i rÃµ hÆ¡n.');
      }
    };

    recognitionRef.current = recognition;
    isInitializedRef.current = true;

    return () => {
      if (timeoutRef.current) {
        clearTimeout(timeoutRef.current);
      }
      if (silenceTimeoutRef.current) {
        clearTimeout(silenceTimeoutRef.current);
      }
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
    };
  }, []);

  const startListening = useCallback(() => {
    console.log('Start listening requested');
    
    if (!hasPermission) {
      setError('KhÃ´ng cÃ³ quyá»n truy cáº­p microphone. Vui lÃ²ng cáº¥p quyá»n trong cÃ i Ä‘áº·t trÃ¬nh duyá»‡t.');
      return;
    }

    setError(null);
    setTranscript('');
    setInterimTranscript('');
    setConfidence(0);
    finalTranscriptRef.current = '';
    hasSpeechDetectedRef.current = false;

    if (recognitionRef.current && !isListening) {
      try {
        recognitionRef.current.start();
      } catch (err) {
        console.error('Speech recognition start error:', err);
        setError('KhÃ´ng thá»ƒ báº¯t Ä‘áº§u nháº­n diá»‡n giá»ng nÃ³i. Vui lÃ²ng thá»­ láº¡i.');
      }
    }
  }, [isListening, hasPermission]);

  const stopListening = useCallback(() => {
    console.log('Stop listening requested');
    
    // Clean up all timeouts
    if (timeoutRef.current) {
      clearTimeout(timeoutRef.current);
      timeoutRef.current = null;
    }
    if (silenceTimeoutRef.current) {
      clearTimeout(silenceTimeoutRef.current);
      silenceTimeoutRef.current = null;
    }

    if (recognitionRef.current && isListening) {
      try {
        recognitionRef.current.stop();
      } catch (err) {
        console.warn('Speech recognition stop error:', err);
      }
    }
    
    setIsListening(false);
    setIsProcessing(false);
  }, [isListening]);

  const resetTranscript = useCallback(() => {
    setTranscript('');
    setInterimTranscript('');
    setConfidence(0);
    setError(null);
    finalTranscriptRef.current = '';
    hasSpeechDetectedRef.current = false;
  }, []);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      if (timeoutRef.current) {
        clearTimeout(timeoutRef.current);
      }
      if (silenceTimeoutRef.current) {
        clearTimeout(silenceTimeoutRef.current);
      }
      if (recognitionRef.current && isListening) {
        recognitionRef.current.stop();
      }
    };
  }, [isListening]);

  return {
    isListening,
    transcript,
    interimTranscript,
    confidence,
    startListening,
    stopListening,
    resetTranscript,
    isSupported: !!(window.SpeechRecognition || window.webkitSpeechRecognition),
    error,
    hasPermission,
    isProcessing,
  };
};